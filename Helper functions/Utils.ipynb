{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Utils.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMthu6n9WOjkn41678Wyiq2"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uJO6sGdALLLN","colab_type":"text"},"source":["# Utils\n","\n","Notebook containing all necessary functions for the conducted analyses"]},{"cell_type":"markdown","metadata":{"id":"mlQ1uY0dLLHs","colab_type":"text"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"Ckgt-0izogiT","colab_type":"code","colab":{}},"source":["! pip install qmcpy --quiet\n","! pip install pytictoc --quiet\n","! pip install POT --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ux96VUALLGg5","colab_type":"code","colab":{}},"source":["import numpy as np\n","import scipy.spatial.distance as distance # distance used for kernel\n","import qmcpy # QMC points\n","from pytictoc import TicToc # timer\n","import ot # Wasserstein distance and Sinkhorn divergence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YIc01F0ANpRf","colab_type":"text"},"source":["## Box-Muller transformation"]},{"cell_type":"markdown","metadata":{"id":"9RC2igG8NyGX","colab_type":"text"},"source":["Box-Muller transformation:"]},{"cell_type":"code","metadata":{"id":"MIMjA1XZNulw","colab_type":"code","colab":{}},"source":["def boxmuller(unif1,unif2):\n","  u1 = np.sqrt(-2*np.log(unif1))*np.cos(2*np.pi*unif2)\n","  u2 = np.sqrt(-2*np.log(unif1))*np.sin(2*np.pi*unif2)\n","  return np.transpose(np.vstack([u1,u2]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0EwcmZhwOSWD","colab_type":"text"},"source":["Function generating standard normals using the box-muller transformation:"]},{"cell_type":"code","metadata":{"id":"8hLV50nGOBoX","colab_type":"code","colab":{}},"source":["def normals(n, d, unif, sv=False):\n","\n","    # avoid origin\n","    unif[unif==0] = np.nextafter(0, 1)\n","\n","    # if d is odd, add one dimension\n","    if d % 2 != 0:\n","      dim = d + 1\n","    else:\n","      dim = d\n","\n","    # expand dimensions for SV model\n","    if sv == True:\n","      dim = 2+2*d\n","\n","    # create standard normal samples\n","    u = np.zeros((n,dim))\n","    for i in np.arange(0,dim,2):\n","      u[:,i:(i+2)] = boxmuller(unif[:,i],unif[:,(i+1)])\n","\n","    # if d is odd, drop one dimension\n","    if d % 2 != 0 or sv == True:\n","      u = np.delete(u,-1,1)\n","\n","    return u"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MmyWGjY4MAuB","colab_type":"text"},"source":["## Kernel"]},{"cell_type":"markdown","metadata":{"id":"CiKVEKGNNRS4","colab_type":"text"},"source":["Gaussian kernel $k(x,y)$, its gradient w.r.t. first element $\\nabla_1k(x,y)$ and its second derivative w.r.t. to the second and first argument $\\nabla_2\\nabla_1k(x,y)$:"]},{"cell_type":"code","metadata":{"id":"1rXFG-97MI0i","colab_type":"code","colab":{}},"source":["def k(x,y,l): \n","\n","    x = x.astype('float32')\n","    y = y.astype('float32')\n","    \n","    # dimensions\n","    d = x.shape[1]\n","    dims = np.arange(d)\n","    \n","    # kernel\n","    kernel = np.exp(-(1/(2*l**2))*distance.cdist(x,y,'sqeuclidean'))\n","    \n","    # first derivative\n","    grad_1 = -1*np.squeeze(np.subtract.outer(x,y)[:,[dims],:,[dims]], axis=0)*(1/l**2)*np.expand_dims(kernel, axis=0)\n","    \n","    #second derivative\n","    grad_21 = (1/l**2)*(np.expand_dims(np.expand_dims(np.eye(d), axis = 2), axis = 3)-np.einsum('ijk,ljk->iljk',np.squeeze(np.subtract.outer(x,y)[:,[dims],:,[dims]], axis=0),np.squeeze(np.subtract.outer(x,y)[:,[dims],:,[dims]], axis=0))*(1/l**2))*kernel\n","    \n","    return list([kernel, grad_1, grad_21])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-jw-IhkMWGX","colab_type":"text"},"source":["## MMD$^2$"]},{"cell_type":"markdown","metadata":{"id":"0_BAVDT_Mek6","colab_type":"text"},"source":["MMD$^2$ approximation:"]},{"cell_type":"code","metadata":{"id":"4kVwc-SbMVQ_","colab_type":"code","colab":{}},"source":["def MMD_approx(n,m,kxx,kxy,kyy):\n","    \n","    # first sum\n","    np.fill_diagonal(kxx, np.repeat(0,n))\n","    sum1 = np.sum(kxx)\n","    \n","    # second sum\n","    sum2 = np.sum(kxy)\n","    \n","    # third sum\n","    np.fill_diagonal(kyy, np.repeat(0,m))\n","    sum3 = np.sum(kyy)\n","    \n","    return (1/(n*(n-1)))*sum1-(2/(m*n))*sum2+(1/(m*(m-1)))*sum3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UeietpK3Mi3P","colab_type":"text"},"source":["MMD$^2$ gradient $\\hat{J}$:"]},{"cell_type":"code","metadata":{"id":"_4fjVkilMwBh","colab_type":"code","colab":{}},"source":["def grad_MMD(p,n,m,grad_g,k1xx,k1xy):\n","    \n","    # first sum\n","    prod1 = np.squeeze(np.einsum('ilj,imjk->lmjk', grad_g, np.expand_dims(k1xx,axis=1)))\n","    if prod1.ndim==2:\n","      np.fill_diagonal(prod1[:,:], 0)\n","      sum1 = np.sum(prod1)\n","    else:\n","      for i in range(p):\n","        np.fill_diagonal(prod1[i,:,:], 0)\n","      sum1 = np.einsum('ijk->i',prod1)\n","    \n","    # second sum\n","    prod2 = np.squeeze(np.einsum('ilj,imjk->lmjk', grad_g, np.expand_dims(k1xy,axis=1)))\n","    if prod2.ndim==2:\n","      sum2 = np.sum(prod2)\n","    else:\n","      sum2 = np.einsum('ijk->i',prod2)\n","    \n","    return (2/(n*(n-1)))*sum1-(2/(n*m))*sum2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1LvGBN7TM8Na","colab_type":"text"},"source":["## Information metric"]},{"cell_type":"markdown","metadata":{"id":"cyS2x9I_NMR0","colab_type":"text"},"source":["Approximation of the informatin metric $g_U(\\theta)$:"]},{"cell_type":"code","metadata":{"id":"QQzAWRijNAfW","colab_type":"code","colab":{}},"source":["def g_approx(p,n,grad_g,k21xx):\n","    \n","    # sum\n","    grad_g_T = np.einsum('ijk -> jik',grad_g)\n","    prod1 = np.einsum('ijk, jlkm -> ilkm', grad_g_T, k21xx)\n","    prod2 = np.einsum('ijkl,jmk->imkl', prod1, grad_g)\n","    for i in range(p):\n","        np.fill_diagonal(prod2[i,i,:,:], 0)\n","    gsum = np.einsum('ijkl->ij', prod2)\n","    \n","    return 1/(n*(n-1))*gsum"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6mXMhuXAMEtl","colab_type":"text"},"source":["## Generators"]},{"cell_type":"markdown","metadata":{"id":"TCpyNp-hNdFA","colab_type":"text"},"source":["Generator $G_\\theta(u)$ and generator gradient $\\nabla_\\theta G_\\theta(u)$ for the **Gaussian location model**:"]},{"cell_type":"code","metadata":{"id":"yM6GBykWMHL-","colab_type":"code","colab":{}},"source":["# generator\n","def gen_gaussian(n, d, unif, theta, sigma):\n","\n","  unif[unif==0] = np.nextafter(0, 1)\n","\n","  # if d is odd, add one dimension\n","  if d % 2 != 0:\n","    dim = d + 1\n","  else:\n","    dim = d\n","\n","  # create standard normal samples\n","  u = np.zeros((n,dim))\n","  for i in np.arange(0,dim,2):\n","    u[:,i:(i+2)] = boxmuller(unif[:,i],unif[:,(i+1)])\n","\n","  # if d is odd, drop one dimension\n","  if d % 2 != 0:\n","    u = np.delete(u,-1,1)\n","\n","  # generate samples\n","  x = theta + u*sigma\n","\n","  return x\n","\n","# gradient of the generator\n","def grad_gen_gaussian(n, theta):\n","    return np.broadcast_to(np.expand_dims(np.eye(theta.shape[0]),axis=2),(theta.shape[0],theta.shape[0],n))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TXlTaktMQY1S","colab_type":"text"},"source":["Generator $G_\\theta(u)$ for the **beta distribution**:"]},{"cell_type":"code","metadata":{"id":"CU6xKop9QbzB","colab_type":"code","colab":{}},"source":["# generator\n","def gen_beta(unif, theta):\n","  unif[unif==0] = np.nextafter(0, 1)\n","  alpha = theta[0]\n","  beta = theta[1]\n","  return np.expand_dims(np.sum(np.log(unif[:,0:(alpha)]),axis=1)/np.sum(np.log(unif[:,0:(alpha+beta)]),axis=1),axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CF4J_XN6PTtV","colab_type":"text"},"source":["Generator $G_\\theta(u)$ and generator gradient $\\nabla_\\theta G_\\theta(u)$ for the **g-and-k distribution**:"]},{"cell_type":"code","metadata":{"id":"upkTi6hvPYdS","colab_type":"code","colab":{}},"source":["# generator\n","def gen_gandk(z, theta):\n","    a = theta[0]\n","    b = theta[1]\n","    g = theta[2]\n","    k = np.exp(theta[3])\n","    g = a+b*(1+0.8*((1-np.exp(-g*z))/(1+np.exp(-g*z))))*((1+z**2)**(k))*z\n","    return g\n","\n","# gradient of the generator\n","def grad_gen_gandk(z,theta):\n","    a = theta[0]\n","    b = theta[1]\n","    g = theta[2]\n","    k = np.exp(theta[3])\n","    grad1 = np.ones(z.shape[0])\n","    grad2 = (1+(4/5)*((1-np.exp(-g*z))/(1+np.exp(-g*z))))*(np.exp(k*np.log(1+z**2)))*z\n","    grad3 = (8/5)*theta[1]*((np.exp(g*z))/(1+np.exp(g*z))**2)*(np.exp(k*np.log(1+z**2)))*z**2\n","    grad4 = b*(1+0.8*((1-np.exp(-g*z))/(1+np.exp(-g*z))))*(np.exp(k*np.log(1+z**2)))*np.log(1+z**2)*z\n","    return np.expand_dims(np.einsum('ij->ji',np.c_[grad1,grad2,grad3,grad4]), axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NdTpJWDjQlof","colab_type":"text"},"source":["Generator $G_\\theta(u)$ and generator gradient $\\nabla_\\theta G_\\theta(u)$ for the **stochastic volatility model**:"]},{"cell_type":"code","metadata":{"id":"PxJ1KzWbQqw4","colab_type":"code","colab":{}},"source":["# generator\n","def gen_sv(u, theta):\n","\n","    t = int((u.shape[0]-1)/2)\n","\n","    # retrieve parameters\n","    phi = (np.exp(theta[0])-1)/(np.exp(theta[0])+1)\n","    kappa = np.exp(theta[1])\n","    sigma = np.exp(theta[2]/2)\n","    \n","    # noise terms\n","    e = u[0:t,:]\n","    h1 = u[t,:]*np.sqrt(sigma**2/(1-phi**2))\n","    eta = u[t+1:,:]*sigma\n","\n","    # t=1\n","    h = h1\n","    y = e[0,:]*kappa*np.exp(0.5*h[0])\n","\n","    # t=2,...,T\n","    for i in range(t-1):\n","      h = np.vstack([h,phi*h[i]+eta[i+1,:]])\n","      y = np.vstack([y,e[i+1,:]*kappa*np.exp(0.5*h[i+1])])\n","\n","    return list([y.T,h.T]) # dimensions: nxT and nxT\n","\n","# generator\n","def grad_gen_sv(u,h,y,theta):\n","\n","  # reparameterisation\n","  phi = (np.exp(theta[0])-1)/(np.exp(theta[0])+1)\n","  sigma = np.exp(theta[2]/2)\n","\n","  n = y.shape[0]\n","  t = y.shape[1]\n","  eta = (u[t+1:,:]*np.exp(theta[2]/2)).T\n","\n","  # theta 1\n","  grad_phi_theta1 = (2*(np.exp(theta[0])))/((np.exp(theta[0])+1)**2)\n","  grad_h_theta1 = ((np.exp(theta[0]/2)-np.exp(-theta[0]/2))/(np.exp(theta[0]/2))+np.exp(-theta[0]/2))*(h[:,0]/2)\n","  for i in range(t-1):\n","    grad_h_theta1 = np.vstack([grad_h_theta1,grad_phi_theta1*h[:,i]+phi*grad_h_theta1[i]])\n","  grad_y_theta1 = y*grad_h_theta1.T/2\n","  grad_y_theta1 = np.expand_dims(grad_y_theta1,axis=2)\n","  \n","  # theta 2\n","  grad_y_theta2 = y\n","  grad_y_theta2 = np.expand_dims(grad_y_theta2,axis=2)\n","\n","  # theta 3\n","  grad_eta_theta3 = eta/2\n","  grad_h_theta3 = h[0,:]/2\n","  for i in range(n-1):\n","    grad_h_theta3 = np.vstack([grad_h_theta3,phi*grad_h_theta3[i]+grad_eta_theta3[i+1,:]])\n","  grad_y_theta3 = y*grad_h_theta3/2\n","  grad_y_theta3 = np.expand_dims(grad_y_theta3,axis=2)\n","\n","  # stack together\n","  grad_stack = np.concatenate((grad_y_theta1,grad_y_theta2,grad_y_theta3), axis=2)\n","\n","  return np.einsum('ijk->jki',grad_stack) # dimensions: Txpxn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txM_du5EWV76","colab_type":"text"},"source":["## Sampling"]},{"cell_type":"markdown","metadata":{"id":"53HlW2e5WZEi","colab_type":"text"},"source":["Function to sample from **Gaussian location model**:\n","\n"]},{"cell_type":"code","metadata":{"id":"SfTJQgrZWfHe","colab_type":"code","colab":{}},"source":["def sample_gaussian(method_sampling,n,d,s,theta):\n","\n","  ' caveat:                                                                 '\n","  ' the qmc or qmc_1 sequence have to be fixed before using this function   '\n","\n","  # odd number of parameters\n","  if d % 2 != 0: \n","    if method_sampling == 'MC':\n","      unif = np.random.rand(n,d+1)\n","    if method_sampling == 'QMC':\n","      unif = qmc_1.gen_samples(n)\n","    if method_sampling == 'RQMC':\n","      unif = qmcpy.Halton(d+1).gen_samples(n)\n","\n","  # even number of parameters\n","  else: \n","    if method_sampling == 'MC':\n","      unif = np.random.rand(n,d)\n","    if method_sampling == 'QMC':\n","      unif = qmc.gen_samples(n)\n","    if method_sampling == 'RQMC':\n","      unif = qmcpy.Halton(d).gen_samples(n)\n","\n","  # use generator  \n","  x = gen_gaussian(n,d,unif,theta,s)\n","\n","  return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PtvjSmfpYDcK","colab_type":"text"},"source":["Function to sample $R$-times for MC and RQMC from **Gaussian location model**:"]},{"cell_type":"code","metadata":{"id":"S9gpGTcxYOIo","colab_type":"code","colab":{}},"source":["def sample_gaussian_r(n,num,d,s,theta):\n","\n","  ' caveat:                                                                 '\n","  ' the qmc or qmc_1 sequence have to be fixed before using this function   '\n","\n","  # MC\n","  x_mc = np.zeros((num,np.max(n),d))\n","  for r in range(num):\n","    # odd number of parameters\n","    if d % 2 != 0: \n","      unif_mc = np.random.rand(np.max(n),d+1)\n","    # even number of parameters\n","    else: \n","      unif_mc = np.random.rand(np.max(n),d)\n","    x = gen_gaussian(np.max(n),d,unif_mc,theta,s)\n","    x_mc[r,:,:] = x\n","\n","  # QMC\n","  # odd number of parameters\n","  if d % 2 != 0: \n","    unif_qmc = qmc_1.gen_samples(np.max(n))\n","  # even number of parameters\n","  else: \n","    unif_qmc = qmc.gen_samples(np.max(n))\n","  x_qmc = gen_gaussian(np.max(n),d,unif_qmc,theta,s)\n","\n","  # RQMC\n","  x_rqmc = np.zeros((num,np.max(n),d))\n","  for r in range(num):\n","    # odd number of parameters\n","    if d % 2 != 0: \n","      unif_rqmc = qmcpy.Halton(d+1).gen_samples(np.max(n))\n","    # even number of parameters\n","    else: \n","      unif_rqmc = qmcpy.Halton(d).gen_samples(np.max(n))\n","    x = gen_gaussian(np.max(n),d,unif_rqmc,theta,s)\n","    x_rqmc[r,:,:] = x\n","\n","  return list([x_mc,x_qmc,x_rqmc])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0iz5bjBslD82","colab_type":"text"},"source":["Function to sample from **beta distribution**:"]},{"cell_type":"code","metadata":{"id":"mpEkf2EalHn4","colab_type":"code","colab":{}},"source":["def sample_beta(method_sampling,n,d,theta):\n","\n","  ' caveat:                                                                 '\n","  ' the qmc sequence has to be fixed before using this function             '\n","  ' as either Halton, Sobol or Lattice points                               '\n","\n","  if method_sampling == 'MC':\n","    unif = np.random.rand(n,theta[0]+theta[1])\n","  if method_sampling == 'QMC':\n","    unif = qmc.gen_samples(np.max(n))\n","  if method_sampling == 'RQMC':\n","    unif = qmcpy.Halton(theta[0]+theta[1]).gen_samples(n)  \n","\n","  # generate samples\n","  x = gen_beta(unif,theta)  \n","\n","  return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d9NiQi9LZApY","colab_type":"text"},"source":["Function to sample $R$-times for MC and RQMC from **beta distribution**:"]},{"cell_type":"code","metadata":{"id":"T_qb9vUgZHTk","colab_type":"code","colab":{}},"source":["def sample_beta_r(qmc_method,n,num,d,theta):\n","\n","  ' caveat:                                                                 '\n","  ' the qmc sequence has to be fixed before using this function             '\n","  ' as either Halton, Sobol or Lattice points                               '\n","\n","  # generate n data points using MC and RQMC\n","  x_mc = np.zeros((num,np.max(n),d))\n","  x_rqmc = np.zeros((num,np.max(n),d))\n","  for r in range(num):\n","    unif_mc = np.random.rand(np.max(n),theta[0]+theta[1])\n","    if qmc_method == 'halton':\n","      unif_rqmc = qmcpy.Halton(theta[0]+theta[1]).gen_samples(np.max(n))\n","    if qmc_method == 'sobol':\n","      unif_rqmc = qmcpy.Sobol(theta[0]+theta[1], graycode=True).gen_samples(np.max(n))\n","    if qmc_method == 'lattice':\n","      unif_rqmc = qmcpy.Lattice(theta[0]+theta[1]).gen_samples(np.max(n))\n","    x_mc[r,:,:] = gen_beta(unif_mc,theta)\n","    x_rqmc[r,:,:] = gen_beta(unif_rqmc,theta)\n","\n","  # QMC\n","  unif_qmc = qmc.gen_samples(np.max(n))\n","  x_qmc = gen_beta(unif_qmc,theta)\n","\n","  return list([x_mc,x_qmc,x_rqmc])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gXZWOL4-aWMc","colab_type":"text"},"source":["Function to sample from **g-and-k distribution**:"]},{"cell_type":"code","metadata":{"id":"nZaSWerPacba","colab_type":"code","colab":{}},"source":["def sample_gandk(method_sampling,n,d,theta):\n","\n","  ' caveat:                                                                 '\n","  ' the qmc sequence has to be fixed before using this function             '\n","\n","  # generate uniforms\n","  if method_sampling == 'MC':\n","    unif = np.random.rand(n,d+1)\n","  if method_sampling == 'QMC':\n","    unif = qmc.gen_samples(n)\n","  if method_sampling == 'RQMC':\n","    unif = qmcpy.Halton(d+1).gen_samples(n)\n","\n","  # generate standard normals  \n","  z = normals(n,d,unif)\n","\n","  # generate samples from g-and-k distribution\n","  x = gen_gandk(z,theta)\n","\n","  return list([x,z])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pKgluUDWbIJ3","colab_type":"text"},"source":["Function to sample $R$-times for MC and RQMC from **g-and-k distribution**:"]},{"cell_type":"code","metadata":{"id":"8qPWkaRDbHH2","colab_type":"code","colab":{}},"source":["def sample_gandk_r(n,num,d,theta):\n","\n","  ' caveat:                                                                 '\n","  ' the qmc sequence has to be fixed before using this function             '\n","\n","  #MC\n","  x_mc = np.zeros((num,np.max(n),d))\n","  for r in range(num):\n","    unif_mc = np.random.rand(np.max(n),d+1)\n","    z = normals(np.max(n), d, unif_mc)\n","    x_mc[r,:,:] = gen_gandk(z,theta)\n","\n","  # QMC\n","  unif_qmc = qmc.gen_samples(np.max(n))\n","  z = normals(np.max(n), d, unif_qmc)\n","  x_qmc = gen_gandk(z,theta)\n","\n","  # RQMC\n","  x_rqmc = np.zeros((num,np.max(n),d))\n","  for r in range(num):\n","    unif_rqmc = qmcpy.Halton(d+1).gen_samples(np.max(n))\n","    z = normals(np.max(n), d, unif_rqmc)\n","    x_rqmc[r,:,:] = gen_gandk(z,theta)\n","\n","  return list([x_mc,x_qmc,x_rqmc])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ixZtgkRgbhtJ","colab_type":"text"},"source":["Function to sample from **SV model**:"]},{"cell_type":"code","metadata":{"id":"83bJO5mCbmk6","colab_type":"code","colab":{}},"source":["def sample_sv(method_sampling,n,d,theta):\n","\n","  ' caveat:                                                                 '\n","  ' the qmc sequence has to be fixed before using this function             '\n","\n","  # generate uniforms\n","  if method_sampling == 'MC':\n","    unif = np.random.rand(n,2+2*d)\n","  if method_sampling == 'QMC':\n","    unif = qmc.gen_samples(n)\n","  if method_sampling == 'RQMC':\n","    unif = qmcpy.Halton(2+2*d).gen_samples(n)\n","\n","  # generate standard normals  \n","  u = normals(n,d,unif,sv=True)\n","\n","  # generate samples from g-and-k distribution and latent variables\n","  x,h = gen_sv(u.T,theta)\n","\n","  return list([x,h,u.T])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lX9XEPEfcvvi","colab_type":"text"},"source":["Function to sample $R$-times for MC and RQMC from **SV model**:"]},{"cell_type":"code","metadata":{"id":"6HsSV_uKcyQG","colab_type":"code","colab":{}},"source":["def sample_sv_r(n,num,d,theta):\n","\n","  ' caveat:                                                                 '\n","  ' the qmc sequence has to be fixed before using this function             '\n","\n","  return list([x_mc,x_qmc,x_rqmc])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sOVrhiWKRPFu","colab_type":"text"},"source":["## Optimisation loop"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LWiP716aFJRc","colab":{}},"source":["def optim(model,method_sampling,method_gd,eta,max_it,l,n,m,d,p,y,start,s=2):\n","\n","  ' caveat:                                                              '\n","  ' the qmc sequence has to be fixed before using this function          '\n","  \n","  ' function arguments:                                                  '   \n","  ' model:           \"gaussian\" or \"gandk\" or \"sv\"                       '\n","  ' method_sampling: \"MC\" or \"QMC\" or \"RQMC\"                             '\n","  ' method_sg:       \"SGD\" or \"NSGD\"                                     '\n","  ' eta:             step size                                           '\n","  ' max_it:          maximum number of iterations                        '\n","  ' l:               lengthscale of the kernel                           '\n","  ' n:               number of samples simulated per iteration           '\n","  ' m:               number of true samples                              '\n","  ' d:               number of dimensions                                '\n","  ' y:               true data set                                       '\n","  ' start:           start values                                        '\n","  ' s (optional):    standard deviation in Gaussian location model       '\n","\n","  # median heuristic if l=-1\n","  if l == -1:\n","    l = np.sqrt((1/2)*np.median(distance.cdist(y,y,'sqeuclidean')))\n","\n","  # pre-define noise for information metric\n","  noise = [0, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n","\n","  # pre-compute the kernel and its derivatives for the true data points\n","  kyy = k(y,y,l)\n","  \n","  # list for squared MMD\n","  loss = []  \n","\n","  # start values\n","  theta = np.expand_dims(start,axis=0) \n","\n","  # create timer instance\n","  t = TicToc()\n","  t.tic()\n","\n","  for i in range(max_it):\n","    \n","    # simulate data using the current estimate for theta\n","    if model == 'gaussian':\n","      x = sample_gaussian(method_sampling,n,d,s,theta[i,:])\n","    if model == 'gandk':\n","      x,z = sample_gandk(method_sampling,n,d,theta[i,:])\n","    if model == 'sv':\n","      x,h,u = sample_sv(method_sampling,n,d,theta[i,:])\n","    \n","    # calculate kernel and the derivatives\n","    kxx = k(x,x,l)\n","    kxy = k(x,y,l)\n","    \n","    # calculate the gradient of the generator\n","    if model == 'gaussian':\n","      grad_g = grad_gen_gaussian(n,theta[i,:])\n","    if model == 'gandk':\n","      grad_g = grad_gen_gandk(z, theta[i,:])\n","    if model == 'sv':\n","      grad_g = grad_gen_sv(u,h,x,theta[i,:])\n","    \n","    # approximate squared MMD gradient\n","    if p==1:\n","      J = np.asmatrix(grad_MMD(p,n,m,grad_g,kxx[1],kxy[1]))\n","    else:\n","      J = grad_MMD(p,n,m,grad_g,kxx[1],kxy[1])\n","    \n","    # approximate information metric\n","    if method_gd == 'NSGD':\n","      g = g_approx(p,n,grad_g,kxx[2])\n","      # add noise if g can't be inverted\n","      for j in range(9):\n","        check = True\n","        try:\n","          np.linalg.inv(g + np.eye(p)*noise[j])\n","        except np.linalg.LinAlgError:\n","          check = False\n","        if check:\n","          break\n","      g = g + np.eye(p)*noise[j]\n","    \n","    # update estimate for theta using NSGD or SGD\n","    if method_gd == 'NSGD':\n","        theta = np.vstack([theta,theta[i,:]-eta*np.linalg.inv(g)@J]) # NSGD\n","    else:\n","        theta = np.vstack([theta,theta[i,:]-eta*J]) # SGD\n","    \n","    # calculate current squared MMD approximation\n","    loss.append(MMD_approx(n,m,kxx[0],kxy[0],kyy[0]))\n","    \n","    # print outputs\n","    if (i+1)%1000 == 0:\n","        print('iteration:',i+1,'\\nloss:     ', round(loss[i],7),'\\nestimate: ',theta[i+1,:],'\\ngradient: ', J)\n","\n","    # stop if nan occurs\n","    if np.isnan(loss[i]):\n","      break\n","\n","  print('-------------------------------------------\\nfinal loss:       ', round(loss[i],7), '\\nfinal estimate:   ', theta[i+1,:],'\\ntotal iterations: ',i+1)\n","  t.toc() \n","\n","  np.savetxt(fname=method_sampling+'_theta.csv', delimiter=\",\", X=theta)\n","  np.savetxt(fname=method_sampling+'_loss.csv', delimiter=\",\", X=loss)\n","\n","  return list([theta, loss])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ia3NwvLMWdM7","colab_type":"text"},"source":["### MSE"]},{"cell_type":"code","metadata":{"id":"7x7DYFCKWecF","colab_type":"code","colab":{}},"source":["def mse(max_it,p,theta1,theta2,theta3,theta_star):\n","  mse1 = np.zeros((max_it-1,p))\n","  mse2 = np.zeros((max_it-1,p))\n","  mse3 = np.zeros((max_it-1,p))\n","  for l in range(p):\n","    for j in range(max_it-1):\n","      mse1[j,l] = np.mean(np.asarray((theta1[1:j+2,l]-theta_star[l]))**2)\n","      mse2[j,l] = np.mean(np.asarray((theta2[1:j+2,l]-theta_star[l]))**2)\n","      mse3[j,l] = np.mean(np.asarray((theta3[1:j+2,l]-theta_star[l]))**2)\n","  return list([mse1,mse2,mse3])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lRnQs6-mdDAN","colab_type":"text"},"source":["## Convergence of MMD$^2$"]},{"cell_type":"markdown","metadata":{"id":"dxNaepO2dIqI","colab_type":"text"},"source":["Function to calculate MMD$^2$ against $n$ for Gaussian location model, g-and-k distribution and SV model:"]},{"cell_type":"code","metadata":{"id":"6p93BJrHdGK8","colab_type":"code","colab":{}},"source":["def mmd_conv(model,n,num,d,y,l,theta,s=2):\n","\n","  # median heuristic if l=-1\n","  if l == -1:\n","    l = np.sqrt((1/2)*np.median(distance.cdist(y,y,'sqeuclidean')))\n","\n","  # kernel\n","  kyy = k(y,y,l)\n","\n","  # generate samples\n","  if model=='gaussian':\n","    x_mc,x_qmc,x_rqmc = sample_gaussian_r(n,num,d,s,theta)\n","  if model == 'gandk':\n","    x_mc,x_qmc,x_rqmc = sample_gandk_r(n,num,d,theta)\n","  if model == 'sv':\n","    x_mc,x_qmc,x_rqmc = sample_sv_r(n,num,d,theta)\n","\n","  # calculate squared MMD for a sequence of n\n","  MMD_mc = []\n","  MMD_qmc = []\n","  MMD_rqmc = []\n","  MMD_min_mc = []\n","  MMD_max_mc = []\n","  MMD_min_rqmc = []\n","  MMD_max_rqmc = []\n","  for j in n:\n","\n","    # R repetitions for MC and RQMC\n","    mc_ave = []\n","    rqmc_ave = []\n","    for r in range(num):\n","      mc_ave.append(MMD_approx(j,m,k(x_mc[r,:j,:],x_mc[r,:j,:],l)[0],k(x_mc[r,:j,:],y,l)[0],kyy[0]))\n","      rqmc_ave.append(MMD_approx(j,m,k(x_rqmc[r,:j,:],x_rqmc[r,:j,:],l)[0],k(x_rqmc[r,:j,:],y,l)[0],kyy[0])) \n","    \n","    # append min and max values for MC and RQMC\n","    MMD_min_mc.append(np.min(np.abs(np.array(mc_ave))))\n","    MMD_max_mc.append(np.max(np.abs(np.array(mc_ave))))\n","    MMD_min_rqmc.append(np.min(np.abs(np.array(rqmc_ave))))\n","    MMD_max_rqmc.append(np.max(np.abs(np.array(rqmc_ave))))\n","\n","    # append value for QMC and mean values for MC and RQMC\n","    MMD_mc.append(np.mean(np.abs(np.array(mc_ave))))\n","    MMD_qmc.append(MMD_approx(j,m,k(x_qmc[:j,:],x_qmc[:j,:],l)[0],k(x_qmc[:j,:],y,l)[0],kyy[0]))\n","    MMD_rqmc.append(np.mean(np.abs(np.array(rqmc_ave))))\n","    print('sample size: ', j)\n","\n","  return list([MMD_mc,MMD_qmc,MMD_rqmc,MMD_min_mc,MMD_max_mc,MMD_min_rqmc,MMD_max_rqmc])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R_pAbo23mH3j","colab_type":"text"},"source":["Function to calculate MMD$^2$ against $n$ for beta distribution using Halton sequence, Sobol' sequences or rank-1 lattice:"]},{"cell_type":"code","metadata":{"id":"-kcVb7tjweWN","colab_type":"code","colab":{}},"source":["# helper to find closest prime number for lattice\n","def find_next_prime(n):\n","  for p in range(n+1, 2*n):\n","        for i in range(2, p):\n","            if p % i == 0:\n","                break\n","        else:\n","            return p\n","  return None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tYV_9e4XmSrJ","colab_type":"code","colab":{}},"source":[" def mmd_conv_beta(qmc_method,n,num,theta,y,l):\n","\n","  # median heuristic if l=-1\n","  if l == -1:\n","    l = np.sqrt((1/2)*np.median(distance.cdist(y,y,'sqeuclidean'))) \n","\n","  # kernel\n","  kyy = k(y,y,l)\n","\n","  # generate samples\n","  x_mc,x_qmc,x_rqmc = sample_beta_r(qmc_method,n,num,d,theta)\n","\n","  # calculate squared MMD for a sequence of n\n","  MMD_mc = []\n","  MMD_qmc = []\n","  MMD_rqmc = []\n","  MMD_min_mc = []\n","  MMD_max_mc = []\n","  MMD_min_rqmc = []\n","  MMD_max_rqmc = []\n","  for j in n:\n","\n","    # R repetitions for MC and RQMC\n","    mc_ave = []\n","    rqmc_ave = []\n","    for r in range(num):\n","      mc_ave.append(MMD_approx(j,m,k(x_mc[r,:j,:],x_mc[r,:j,:],l)[0],k(x_mc[r,:j,:],y,l)[0],kyy[0]))\n","      rqmc_ave.append(MMD_approx(j,m,k(x_rqmc[r,:j,:],x_rqmc[r,:j,:],l)[0],k(x_rqmc[r,:j,:],y,l)[0],kyy[0]))\n","\n","    # append min and max values for MC and RQMC\n","    MMD_min_mc.append(np.min(np.abs(np.array(mc_ave))))\n","    MMD_max_mc.append(np.max(np.abs(np.array(mc_ave))))\n","    MMD_min_rqmc.append(np.min(np.abs(np.array(rqmc_ave))))\n","    MMD_max_rqmc.append(np.max(np.abs(np.array(rqmc_ave)))) \n","\n","    # append value for QMC and mean values for MC and RQMC\n","    MMD_mc.append(np.mean(np.abs(np.array(mc_ave))))\n","    MMD_qmc.append(MMD_approx(j,m,k(x_qmc[:j,:],x_qmc[:j,:],l)[0],k(x_qmc[:j,:],y,l)[0],kyy[0]))\n","    MMD_rqmc.append(np.mean(np.abs(np.array(rqmc_ave))))\n","\n","    print('sample size: ', j)\n","\n","  return list([MMD_mc,MMD_qmc,MMD_rqmc,MMD_min_mc,MMD_max_mc,MMD_min_rqmc,MMD_max_rqmc])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"64W_HOMQ-arO","colab_type":"text"},"source":["### Convergence of 1-Wasserstein distance"]},{"cell_type":"markdown","metadata":{"id":"sUq7P3DzAf3W","colab_type":"text"},"source":["Function to calculate 1-Wasserstein distance against $n$ for Gaussian location model, g-and-k distribution and SV model:"]},{"cell_type":"code","metadata":{"id":"ETAEbzx9-d_R","colab_type":"code","colab":{}},"source":[" def W_conv(model,n,m,num,d,y,theta,s=2): \n","\n","  # generate samples\n","  if model=='gaussian':\n","    x_mc,x_qmc,x_rqmc = sample_gaussian_r(n,num,d,s,theta)\n","  if model == 'gandk':\n","    x_mc,x_qmc,x_rqmc = sample_gandk_r(n,num,d,theta)\n","  if model == 'sv':\n","    x_mc,x_qmc,x_rqmc = sample_sv_r(n,num,d,theta)\n","\n","  # calculate Wasserstein distance for a sequence of n\n","  W_mc = []\n","  W_qmc = []\n","  W_rqmc = []\n","  W_min_mc = []\n","  W_max_mc = []\n","  W_min_rqmc = []\n","  W_max_rqmc = []\n","  for j in n:\n","\n","    mc_ave = []\n","    rqmc_ave = []\n","\n","    # equal weights\n","    a = np.ones((j,)) / j \n","    b = np.ones((m,)) / m\n","    \n","    # MC and RQMC\n","    for r in range(num):\n","      M = ot.dist(x_mc[r,:j,:], y, 'euclidean')\n","      M /= M.max()\n","      mc_ave.append(ot.emd2(a, b, M))\n","      M = ot.dist(x_rqmc[r,:j,:], y, 'euclidean')\n","      M /= M.max()\n","      rqmc_ave.append(ot.emd2(a, b, M))\n","    W_mc.append(np.mean(np.abs(np.array(mc_ave))))\n","    W_rqmc.append(np.mean(np.abs(np.array(rqmc_ave))))\n","    \n","    # append min and max values for MC and RQMC\n","    W_min_mc.append(np.min(np.abs(np.array(mc_ave))))\n","    W_max_mc.append(np.max(np.abs(np.array(mc_ave))))\n","    W_min_rqmc.append(np.min(np.abs(np.array(rqmc_ave))))\n","    W_max_rqmc.append(np.max(np.abs(np.array(rqmc_ave)))) \n","\n","    # QMC\n","    M = ot.dist(x_qmc[:j,:], y, 'euclidean')\n","    M /= M.max()\n","    W_qmc.append(np.abs(ot.emd2(a, b, M)))\n","\n","    print('sample size: ', j)\n","\n","  return list([W_mc,W_qmc,W_rqmc,W_min_mc,W_max_mc,W_min_rqmc,W_max_rqmc])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mdc9IAS8AnVk","colab_type":"text"},"source":["### Convergence of Sinkhorn loss"]},{"cell_type":"markdown","metadata":{"id":"M6ztPL45Asks","colab_type":"text"},"source":["Function to calculate Sinkhorn loss against $n$ for Gaussian location model, g-and-k distribution and SV model:"]},{"cell_type":"code","metadata":{"id":"ZBRJmPiLArv8","colab_type":"code","colab":{}},"source":["def sink_conv(model,n,m,num,d,y,theta,e,s=2):\n","\n","  # Sinkhorn divergence for y,y\n","  b = np.ones((m,)) / m\n","  M = ot.dist(y, y, 'sqeuclidean')\n","  M /= M.max()\n","  sinkyy = ot.sinkhorn2(b, b, M, e)\n","\n","  # generate samples\n","  if model=='gaussian':\n","    x_mc,x_qmc,x_rqmc = sample_gaussian_r(n,num,d,s,theta)\n","  if model == 'gandk':\n","    x_mc,x_qmc,x_rqmc = sample_gandk_r(n,num,d,theta)\n","  if model == 'sv':\n","    x_mc,x_qmc,x_rqmc = sample_sv_r(n,num,d,theta)\n","\n","  # calculate Sinkhorn distance for a sequence of n\n","  sinkxy_qmc = []\n","  sinkxx_qmc = []\n","  sink_mc = []\n","  sink_rqmc = []\n","  sink_min_mc = []\n","  sink_max_mc = []\n","  sink_min_rqmc = []\n","  sink_max_rqmc = []\n","  for j in n:\n","\n","    # equal weights\n","    a = np.ones((j,)) / j  \n","\n","    #MC\n","    mc_avexy = []\n","    mc_avexx = []\n","    for r in range(num):\n","      M = ot.dist(x_mc[r,:j,:], y, 'sqeuclidean')\n","      M /= M.max()\n","      mc_avexy.append(ot.sinkhorn2(a, b, M, e))\n","      M = ot.dist(x_mc[r,:j,:], x_mc[r,:j,:], 'sqeuclidean')\n","      M /= M.max()\n","      mc_avexx.append(ot.sinkhorn2(a, a, M, e))\n","\n","    # QMC\n","    M = ot.dist(x_qmc[:j,:], y, 'sqeuclidean')\n","    M /= M.max()\n","    sinkxy_qmc.append(ot.sinkhorn2(a, b, M, e))\n","    M = ot.dist(x_qmc[:j,:], x_qmc[:j,:], 'sqeuclidean')\n","    M /= M.max()\n","    sinkxx_qmc.append(ot.sinkhorn2(a, a, M, e))\n","\n","    # RQMC\n","    rqmc_avexy = []\n","    rqmc_avexx = []\n","    for r in range(num):\n","      M = ot.dist(x_rqmc[r,:j,:], y, 'sqeuclidean')\n","      M /= M.max()\n","      rqmc_avexy.append(ot.sinkhorn2(a, b, M, e))\n","      M = ot.dist(x_rqmc[r,:j,:], x_rqmc[r,:j,:], 'sqeuclidean')\n","      M /= M.max()\n","      rqmc_avexx.append(ot.sinkhorn2(a, a, M, e))\n","\n","    # calculate mean Sinkhorn loss for MC and RQMC\n","    sink_ave_mc = np.abs(2*np.array(mc_avexy)-np.array(mc_avexx)-sinkyy)\n","    sink_ave_rqmc = np.abs(2*np.array(rqmc_avexy)-np.array(rqmc_avexx)-sinkyy)\n","    sink_mc.append(np.mean(sink_ave_mc))\n","    sink_rqmc.append(np.mean(sink_ave_rqmc))\n","\n","    # calculate min and max values for MC and RQMC\n","    sink_max_mc.append(np.max(sink_ave_mc))\n","    sink_min_mc.append(np.min(sink_ave_mc))\n","    sink_max_rqmc.append(np.max(sink_ave_rqmc))\n","    sink_min_rqmc.append(np.min(sink_ave_rqmc))\n","\n","    print('sample size: ', j)\n","\n","  # calculate Sinkhorn loss for QMC\n","  sink_qmc = np.abs(2*np.array(sinkxy_qmc)-np.array(sinkxx_qmc)-sinkyy)\n","\n","  return list([sink_mc,sink_qmc,sink_rqmc,sink_min_mc,sink_max_mc,sink_min_rqmc,sink_max_rqmc])"],"execution_count":null,"outputs":[]}]}